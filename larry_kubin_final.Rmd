---
title: "COMPSCIX 415.2 Homework 9/Final"
author: "Larry Kubin"
date: "April 5, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Bootstrapping

### 1. Steps 

#### Load the train.csv dataset into R.

```{r}
library(tidyverse)
training_data = read.csv("train.csv")
```

#### Convert all character columns into unordered factors.

The character columns are already factors by default. See glimpse below.

```{r}
# Showing that Embarked already has levels
levels(training_data$Embarked)
```

#### Convert the Survived column into an unordered factor because it is loaded as an integer by default.

```{r}
training_data$Survived = factor(training_data$Survived)
```

#### Take a glimpse of your data to confirm that all of the columns were converted correctly

```{r}
glimpse(training_data)
```

### 2. Use the code below to take 100 bootstrap samples of your data. Confirm that the result is a tibble with a list column of resample objects - each resample object is a bootstrap sample of the titanic dataset.

```{r}
library(tidyverse)
library(modelr)
titanic_boot <- bootstrap(training_data, 100)

class(titanic_boot)

glimpse(titanic_boot)
```

### 3. Confirm that some of your bootstrap samples are in fact bootstrap samples (meaning they should have some rows that are repeated). You can use the n_distinct() function from dplyr to see that your samples have different numbers of unique rows. Use the code below to help you extract some of the resample objects from the strap column (which is an R list), convert them to tibbles, and then count distinct rows. Use the code below, no changes necessary.

```{r}
# since the strap column of titanic_boot is a list, we can
# extract the resampled data using the double brackets [[]],
# and just pick out a few of them to compare the number of
# distinct rows
as.tibble(titanic_boot$strap[[1]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[2]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[3]]) %>% n_distinct()
```

### 4. Now, let’s demonstrate the Central Limit Theorem using the Age column. We’ll iterate through all 100 bootstrap samples, take the mean of Age, and collect the results.
• We will define our own function to pull out the mean of Age from each bootstrap sample and
• create our own for loop to iterate through.
Use the code below and fill in the blanks.

```{r}
age_mean <- function(ages) {
  data <- as.tibble(ages) 
  mean_age <- mean(data$Age, na.rm = TRUE) # take the mean of Age, remove NAs
  return(mean_age)
}
# loop through the 100 bootstrap samples and use the age_mean()
# function
all_means <- rep(NA, 100)

# start the loop
for(i in 1:100) {
  all_means[i] <- age_mean(titanic_boot$strap[[i]])
}
# take a look at some of the means you calculated from your samples
head(all_means)

# convert to a tibble so we can use if for plotting
all_means <- tibble(all_means = all_means)

class(all_means)
glimpse(all_means)
```

### 5. Plot a histogram of all_means.

```{r}
ggplot() + aes(all_means) + geom_histogram(binwidth=.1, colour="black", fill="white")
```

### 6. Find the standard error of the sample mean of Age using your boostrap sample means. Compare the empirical standard error to the theoretical standard error. Recall that the theoretical standard error is given by:

```{r}
standard_error = sapply(all_means, sd) / 100
print(standard_error)
```

# Random Forest

### 1. Randomly split your data into training and testing using the code below so that we all have the same sets.

```{r}
set.seed(987)
model_data <- resample_partition(training_data, c(test = 0.3, train = 0.7))
train_set <- as.tibble(model_data$train)
test_set <- as.tibble(model_data$test)
```

## 2. Fit a decision tree to train_set using the rpart package, and using Pclass, Sex, Age, SibSp, Parch, Fare, Embarked as the features.

### Plot the tree using the partykit package.

### What do you notice about this tree compared to the one from last week which only contained three features?

```{r}
library(rpart)
library(partykit)

dtree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
               data=train_set, method="class")

plot(as.party(dtree))
```

### 3. Fit a random forest to train_set using the randomForest package, and using Pclass, Sex, Age, SibSp, Parch, Fare, Embarked as the features. We’ll use 500 trees and sample four features at each split. Use the code below and fill in the blanks.

```{r}
library(randomForest)
rf_mod <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data = train_set,
ntrees = 500,
mtry = 4,
na.action = na.roughfix)
```


### 4. Compare the performance of the decision tree with the random forest using the ROCR package and the AUC. Which model performs the best?

Here’s some code to get you started.

```{r}
library(ROCR)
rf_preds <- predict(rf_mod, newdata = test_set, type = 'prob')[,2]
tree_preds <- predict(dtree, newdata = test_set)[,2]
pred_rf <- prediction(predictions = rf_preds, labels = test_set$Survived)
pred_tree <- prediction(predictions = tree_preds, labels = test_set$Survived)

print("Decision Tree Performance")
tree_performance <- performance(pred_tree, measure = "auc")
print(tree_performance@y.values)

print("Random Forest Performance")
rf_performance = performance(pred_rf, measure = "auc")
print(rf_performance@y.values)
```
Random forest performs better.

### 5. Plot the ROC curves for the decision tree and the random forest above, on the same plot with a legend that differentiates and specifies which curve belongs to which model. Use the code below to get you started.

Hints:

You will have to modify the plot_roc() function to plot the two curves together with different colors and a legend. This is easier to do if the data for plotting the two curves are in one tibble. You can combine tibbles using the bind_rows() function.

```{r}
# get the FPR and TPR for the logistic model
# recall that the ROC curve plots the FPR on the x-axis
perf_rf <- performance(pred_rf, measure = 'tpr', x.measure = 'fpr')
perf_rf_tbl <- tibble(perf_rf@x.values[[1]], perf_rf@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_rf_tbl) <- c('fpr', 'tpr')
# get the FPR and TPR for the tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])
# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')

# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
  p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr)) +
  geom_line(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, lty = 3) +
  labs(x = 'False positive rate', y = 'True positive rate') +
  theme_bw()
  return(p)
}
```

### 6. Answer these questions about the ROC curves:

#### Which model performs better: decision tree or random forest?

Random Forest

#### What is the approximate false positive rate, for both the decision tree and the random forest, if we attain a true positive rate of approximately 0.75? Answers do not need to be exact - just ballpark it by looking at the plots.
